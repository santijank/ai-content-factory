"""
Integration Test Configuration
Handles test environment setup and configuration for integration testing
"""

import os
import json
import logging
from pathlib import Path
from typing import Dict, Any, Optional
from dataclasses import dataclass
from unittest.mock import Mock, patch

# Suppress logs during testing
logging.getLogger().setLevel(logging.WARNING)

@dataclass
class TestConfig:
    """Test configuration settings"""
    
    # Database settings
    test_db_url: str = "sqlite:///:memory:"
    use_real_db: bool = False
    
    # API settings
    api_base_url: str = "http://localhost:5000"
    api_timeout: int = 30
    
    # AI Service settings
    mock_ai_services: bool = True
    groq_api_key: str = "test-groq-key"
    openai_api_key: str = "test-openai-key"
    
    # External service settings
    mock_external_apis: bool = True
    youtube_api_key: str = "test-youtube-key"
    google_trends_enabled: bool = False
    
    # Performance settings
    max_response_time: float = 5.0
    min_requests_per_second: float = 10.0
    stress_test_duration: int = 10
    
    # Coverage settings
    min_coverage_percentage: float = 80.0
    generate_coverage_report: bool = True
    
    # Report settings
    generate_html_report: bool = True
    report_directory: str = "test_reports"
    
    # Security test settings
    enable_security_tests: bool = True
    test_sql_injection: bool = True
    test_xss_prevention: bool = True
    
    @classmethod
    def from_env(cls) -> 'TestConfig':
        """Create config from environment variables"""
        return cls(
            test_db_url=os.getenv('TEST_DATABASE_URL', cls.test_db_url),
            use_real_db=os.getenv('USE_REAL_DB', 'false').lower() == 'true',
            api_base_url=os.getenv('TEST_API_BASE_URL', cls.api_base_url),
            api_timeout=int(os.getenv('API_TIMEOUT', str(cls.api_timeout))),
            mock_ai_services=os.getenv('MOCK_AI_SERVICES', 'true').lower() == 'true',
            groq_api_key=os.getenv('GROQ_API_KEY', cls.groq_api_key),
            openai_api_key=os.getenv('OPENAI_API_KEY', cls.openai_api_key),
            mock_external_apis=os.getenv('MOCK_EXTERNAL_APIS', 'true').lower() == 'true',
            youtube_api_key=os.getenv('YOUTUBE_API_KEY', cls.youtube_api_key),
            max_response_time=float(os.getenv('MAX_RESPONSE_TIME', str(cls.max_response_time))),
            min_requests_per_second=float(os.getenv('MIN_RPS', str(cls.min_requests_per_second))),
            stress_test_duration=int(os.getenv('STRESS_TEST_DURATION', str(cls.stress_test_duration))),
            min_coverage_percentage=float(os.getenv('MIN_COVERAGE', str(cls.min_coverage_percentage))),
        )

class MockServiceManager:
    """Manages mock services for testing"""
    
    def __init__(self, config: TestConfig):
        self.config = config
        self.active_mocks = []
    
    def setup_ai_service_mocks(self):
        """Setup AI service mocks"""
        if not self.config.mock_ai_services:
            return
        
        # Mock Groq service
        groq_mock = patch('content_engine.ai_services.text_ai.groq_service.GroqService')
        groq_instance = groq_mock.start()
        
        groq_instance.return_value.generate_content_plan.return_value = {
            "content_type": "educational",
            "title": "Mock AI Generated Title",
            "description": "Mock AI generated description for testing",
            "script": {
                "hook": "Attention-grabbing hook generated by AI",
                "main_content": "Main content explaining the topic in detail",
                "cta": "Strong call-to-action to engage viewers"
            },
            "visual_plan": {
                "style": "realistic",
                "scenes": [
                    "Opening scene with title overlay",
                    "Main content visualization", 
                    "Call-to-action scene"
                ],
                "text_overlays": ["Hook Text", "Key Point", "CTA"]
            },
            "audio_plan": {
                "voice_style": "professional",
                "background_music": "upbeat",
                "sound_effects": ["transition", "emphasis"]
            },
            "platform_optimization": {
                "youtube_title": "Mock YouTube Optimized Title",
                "tiktok_caption": "Mock TikTok caption #viral #test",
                "hashtags": ["#test", "#mock", "#ai"]
            },
            "production_estimate": {
                "time_minutes": 25,
                "cost_baht": 20,
                "complexity": "medium"
            }
        }
        
        self.active_mocks.append(groq_mock)
        
        # Mock OpenAI service
        openai_mock = patch('content_engine.ai_services.text_ai.openai_service.OpenAIService')
        openai_instance = openai_mock.start()
        
        openai_instance.return_value.generate_content_plan.return_value = {
            "content_type": "entertainment",
            "title": "Premium AI Generated Content",
            "description": "High-quality content generated with premium AI",
            "script": {
                "hook": "Premium hook with advanced AI",
                "main_content": "Detailed premium content",
                "cta": "Premium call-to-action"
            }
        }
        
        self.active_mocks.append(openai_mock)
        
        # Mock image generation services
        sd_mock = patch('content_engine.ai_services.image_ai.stable_diffusion.StableDiffusionService')
        sd_instance = sd_mock.start()
        sd_instance.return_value.generate_image.return_value = b"mock_image_data"
        
        self.active_mocks.append(sd_mock)
        
        print("âœ… AI service mocks activated")
    
    def setup_external_api_mocks(self):
        """Setup external API mocks"""
        if not self.config.mock_external_apis:
            return
        
        # Mock YouTube trends
        youtube_mock = patch('trend_monitor.services.youtube_trends.YouTubeTrends.get_trending')
        youtube_mock.start().return_value = [
            {
                'title': 'Mock Trending Video 1',
                'views': 1500000,
                'growth_rate': 25.5,
                'category': 'entertainment',
                'published_at': '2024-01-15T10:30:00Z'
            },
            {
                'title': 'Mock Trending Video 2', 
                'views': 850000,
                'growth_rate': 18.2,
                'category': 'technology',
                'published_at': '2024-01-15T14:20:00Z'
            }
        ]
        
        self.active_mocks.append(youtube_mock)
        
        # Mock Google Trends
        google_trends_mock = patch('trend_monitor.services.google_trends.GoogleTrends.get_trending')
        google_trends_mock.start().return_value = [
            {
                'query': 'Mock trending search 1',
                'interest': 85,
                'region': 'thailand',
                'category': 'technology'
            },
            {
                'query': 'Mock trending search 2',
                'interest': 72,
                'region': 'thailand', 
                'category': 'entertainment'
            }
        ]
        
        self.active_mocks.append(google_trends_mock)
        
        # Mock platform upload services
        platform_mock = patch('platform_manager.services.platform_manager.PlatformManager.upload_content')
        platform_mock.start().return_value = {
            'success': True,
            'platform_id': 'mock_upload_123',
            'url': 'https://mock-platform.com/video/mock_upload_123',
            'status': 'uploaded'
        }
        
        self.active_mocks.append(platform_mock)
        
        print("âœ… External API mocks activated")
    
    def cleanup_mocks(self):
        """Cleanup all active mocks"""
        for mock in self.active_mocks:
            mock.stop()
        
        self.active_mocks.clear()
        print("ðŸ§¹ All mocks cleaned up")

class TestDataGenerator:
    """Generates test data for integration tests"""
    
    @staticmethod
    def generate_sample_trends(count: int = 10) -> list:
        """Generate sample trend data"""
        import random
        from datetime import datetime, timedelta
        
        categories = ['technology', 'entertainment', 'education', 'news', 'lifestyle']
        sources = ['youtube', 'google', 'tiktok', 'twitter']
        
        trends = []
        
        for i in range(count):
            trend = {
                'source': random.choice(sources),
                'topic': f'Test Trend {i+1}: {random.choice(["AI Innovation", "Social Media", "Gaming", "Food", "Travel"])}',
                'keywords': [f'keyword{i}', f'test{i}', random.choice(['viral', 'trending', 'popular'])],
                'popularity_score': round(random.uniform(1.0, 10.0), 1),
                'growth_rate': round(random.uniform(0.0, 50.0), 1),
                'category': random.choice(categories),
                'region': random.choice(['thailand', 'global', 'asia']),
                'collected_at': datetime.now() - timedelta(hours=random.randint(1, 24))
            }
            trends.append(trend)
        
        return trends
    
    @staticmethod
    def generate_sample_opportunities(trends: list, count: int = 5) -> list:
        """Generate sample opportunities from trends"""
        import random
        
        opportunities = []
        
        for i in range(min(count, len(trends))):
            trend = trends[i]
            
            opportunity = {
                'trend_id': trend.get('id', f'trend_{i}'),
                'suggested_angle': f'Content Angle {i+1}: How to leverage {trend["topic"][:30]}...',
                'estimated_views': random.randint(10000, 100000),
                'competition_level': random.choice(['low', 'medium', 'high']),
                'production_cost': round(random.uniform(15.0, 50.0), 2),
                'estimated_roi': round(random.uniform(1.5, 5.0), 1),
                'priority_score': round(random.uniform(5.0, 10.0), 1),
                'status': 'pending'
            }
            opportunities.append(opportunity)
        
        return opportunities
    
    @staticmethod
    def generate_performance_metrics(upload_id: str, days: int = 7) -> list:
        """Generate performance metrics over time"""
        import random
        from datetime import datetime, timedelta
        
        metrics = []
        base_views = random.randint(1000, 10000)
        
        for i in range(days):
            date = datetime.now() - timedelta(days=i)
            
            # Simulate viral growth or decline
            growth_factor = random.uniform(0.8, 1.5)
            views = int(base_views * growth_factor * (1 + i * 0.1))
            
            metric = {
                'upload_id': upload_id,
                'views': views,
                'likes': int(views * random.uniform(0.03, 0.08)),  # 3-8% like rate
                'comments': int(views * random.uniform(0.005, 0.02)),  # 0.5-2% comment rate
                'shares': int(views * random.uniform(0.001, 0.01)),  # 0.1-1% share rate
                'revenue': round(views * random.uniform(0.0005, 0.002), 2),  # Revenue per view
                'cost': round(random.uniform(20.0, 40.0), 2),
                'measured_at': date
            }
            metrics.append(metric)
        
        return metrics

class TestEnvironmentManager:
    """Manages test environment setup and teardown"""
    
    def __init__(self, config: TestConfig):
        self.config = config
        self.mock_manager = MockServiceManager(config)
        self.temp_files = []
        self.test_db_session = None
    
    def setup(self):
        """Setup test environment"""
        print("ðŸ”§ Setting up test environment...")
        
        # Setup database
        self._setup_database()
        
        # Setup mocks
        if self.config.mock_ai_services:
            self.mock_manager.setup_ai_service_mocks()
        
        if self.config.mock_external_apis:
            self.mock_manager.setup_external_api_mocks()
        
        # Setup directories
        self._setup_directories()
        
        # Setup environment variables
        self._setup_environment_variables()
        
        print("âœ… Test environment ready")
    
    def teardown(self):
        """Teardown test environment"""
        print("ðŸ§¹ Tearing down test environment...")
        
        # Cleanup mocks
        self.mock_manager.cleanup_mocks()
        
        # Cleanup database
        if self.test_db_session:
            self.test_db_session.close()
        
        # Cleanup temporary files
        self._cleanup_temp_files()
        
        print("âœ… Test environment cleaned up")
    
    def _setup_database(self):
        """Setup test database"""
        try:
            from database.models.base import init_db, get_db_session
            
            # Initialize database
            init_db()
            
            # Create session for test data
            self.test_db_session = get_db_session()
            
            print("âœ… Test database initialized")
            
        except Exception as e:
            print(f"âŒ Database setup failed: {e}")
            raise
    
    def _setup_directories(self):
        """Setup required directories"""
        directories = [
            self.config.report_directory,
            'temp_test_files',
            'test_uploads',
            'test_content'
        ]
        
        for directory in directories:
            Path(directory).mkdir(exist_ok=True)
    
    def _setup_environment_variables(self):
        """Setup environment variables for testing"""
        env_vars = {
            'FLASK_ENV': 'testing',
            'DATABASE_URL': self.config.test_db_url,
            'SECRET_KEY': 'test-secret-key',
            'GROQ_API_KEY': self.config.groq_api_key,
            'OPENAI_API_KEY': self.config.openai_api_key,
            'YOUTUBE_API_KEY': self.config.youtube_api_key,
            'TESTING': 'true'
        }
        
        for key, value in env_vars.items():
            os.environ[key] = value
    
    def _cleanup_temp_files(self):
        """Cleanup temporary files"""
        temp_directories = ['temp_test_files', 'test_uploads', 'test_content']
        
        for directory in temp_directories:
            dir_path = Path(directory)
            if dir_path.exists():
                import shutil
                shutil.rmtree(dir_path, ignore_errors=True)
    
    def populate_test_data(self):
        """Populate database with test data"""
        try:
            from database.repositories.trend_repository import TrendRepository
            from database.repositories.opportunity_repository import OpportunityRepository
            
            trend_repo = TrendRepository()
            opp_repo = OpportunityRepository()
            
            # Generate and insert test trends
            trends_data = TestDataGenerator.generate_sample_trends(20)
            created_trends = []
            
            for trend_data in trends_data:
                trend = trend_repo.create_trend(trend_data)
                if trend:
                    created_trends.append(trend)
            
            # Generate and insert test opportunities
            opportunities_data = TestDataGenerator.generate_sample_opportunities(
                [{'id': t.id, 'topic': t.topic} for t in created_trends], 10
            )
            
            for opp_data in opportunities_data:
                opp_repo.create_opportunity(opp_data)
            
            print(f"âœ… Test data populated: {len(created_trends)} trends, {len(opportunities_data)} opportunities")
            
        except Exception as e:
            print(f"âŒ Failed to populate test data: {e}")
            raise

class TestResultCollector:
    """Collects and analyzes test results"""
    
    def __init__(self):
        self.results = {}
        self.start_time = None
        self.end_time = None
    
    def start_collection(self):
        """Start collecting test results"""
        self.start_time = datetime.now()
        print("ðŸ“Š Started test result collection")
    
    def add_result(self, test_name: str, success: bool, duration: float, details: Dict[str, Any] = None):
        """Add test result"""
        self.results[test_name] = {
            'success': success,
            'duration': duration,
            'details': details or {},
            'timestamp': datetime.now().isoformat()
        }
    
    def finish_collection(self):
        """Finish collecting test results"""
        self.end_time = datetime.now()
        print("ðŸ“Š Finished test result collection")
    
    def get_summary(self) -> Dict[str, Any]:
        """Get test results summary"""
        total_tests = len(self.results)
        passed_tests = sum(1 for result in self.results.values() if result['success'])
        failed_tests = total_tests - passed_tests
        
        total_duration = (self.end_time - self.start_time).total_seconds() if self.end_time and self.start_time else 0
        
        return {
            'total_tests': total_tests,
            'passed_tests': passed_tests,
            'failed_tests': failed_tests,
            'success_rate': (passed_tests / total_tests) * 100 if total_tests > 0 else 0,
            'total_duration': total_duration,
            'start_time': self.start_time.isoformat() if self.start_time else None,
            'end_time': self.end_time.isoformat() if self.end_time else None
        }
    
    def get_failed_tests(self) -> Dict[str, Dict[str, Any]]:
        """Get failed test details"""
        return {
            name: result for name, result in self.results.items() 
            if not result['success']
        }
    
    def export_results(self, output_file: str):
        """Export results to JSON file"""
        export_data = {
            'summary': self.get_summary(),
            'results': self.results,
            'failed_tests': self.get_failed_tests()
        }
        
        with open(output_file, 'w') as f:
            json.dump(export_data, f, indent=2, default=str)
        
        print(f"ðŸ“„ Test results exported to {output_file}")

class APITestClient:
    """Enhanced test client for API testing"""
    
    def __init__(self, base_url: str, timeout: int = 30):
        self.base_url = base_url.rstrip('/')
        self.timeout = timeout
        self.session_headers = {}
    
    def set_header(self, key: str, value: str):
        """Set persistent header for all requests"""
        self.session_headers[key] = value
    
    def get(self, endpoint: str, params: Dict[str, Any] = None, headers: Dict[str, str] = None):
        """Make GET request"""
        import requests
        
        url = f"{self.base_url}{endpoint}"
        request_headers = {**self.session_headers, **(headers or {})}
        
        try:
            response = requests.get(
                url, 
                params=params, 
                headers=request_headers, 
                timeout=self.timeout
            )
            return response
        except requests.exceptions.RequestException as e:
            raise Exception(f"GET request failed: {e}")
    
    def post(self, endpoint: str, json_data: Dict[str, Any] = None, headers: Dict[str, str] = None):
        """Make POST request"""
        import requests
        
        url = f"{self.base_url}{endpoint}"
        request_headers = {
            'Content-Type': 'application/json',
            **self.session_headers, 
            **(headers or {})
        }
        
        try:
            response = requests.post(
                url, 
                json=json_data, 
                headers=request_headers, 
                timeout=self.timeout
            )
            return response
        except requests.exceptions.RequestException as e:
            raise Exception(f"POST request failed: {e}")
    
    def health_check(self) -> bool:
        """Check if API is healthy"""
        try:
            response = self.get('/api/stats')
            return response.status_code == 200
        except:
            return False

class PerformanceMonitor:
    """Monitors performance during tests"""
    
    def __init__(self):
        self.metrics = []
        self.monitoring = False
    
    def start_monitoring(self):
        """Start performance monitoring"""
        import threading
        import time
        import psutil
        
        self.monitoring = True
        
        def monitor():
            while self.monitoring:
                try:
                    cpu_percent = psutil.cpu_percent()
                    memory = psutil.virtual_memory()
                    
                    metric = {
                        'timestamp': time.time(),
                        'cpu_percent': cpu_percent,
                        'memory_percent': memory.percent,
                        'memory_used_mb': memory.used / (1024 * 1024)
                    }
                    self.metrics.append(metric)
                    
                    time.sleep(1)  # Monitor every second
                except:
                    break
        
        monitor_thread = threading.Thread(target=monitor, daemon=True)
        monitor_thread.start()
        
        print("ðŸ“ˆ Performance monitoring started")
    
    def stop_monitoring(self):
        """Stop performance monitoring"""
        self.monitoring = False
        print("ðŸ“ˆ Performance monitoring stopped")
    
    def get_peak_usage(self) -> Dict[str, float]:
        """Get peak resource usage"""
        if not self.metrics:
            return {'cpu_percent': 0, 'memory_percent': 0, 'memory_used_mb': 0}
        
        peak_cpu = max(m['cpu_percent'] for m in self.metrics)
        peak_memory_percent = max(m['memory_percent'] for m in self.metrics)
        peak_memory_mb = max(m['memory_used_mb'] for m in self.metrics)
        
        return {
            'cpu_percent': peak_cpu,
            'memory_percent': peak_memory_percent,
            'memory_used_mb': peak_memory_mb
        }
    
    def get_average_usage(self) -> Dict[str, float]:
        """Get average resource usage"""
        if not self.metrics:
            return {'cpu_percent': 0, 'memory_percent': 0, 'memory_used_mb': 0}
        
        avg_cpu = sum(m['cpu_percent'] for m in self.metrics) / len(self.metrics)
        avg_memory_percent = sum(m['memory_percent'] for m in self.metrics) / len(self.metrics)
        avg_memory_mb = sum(m['memory_used_mb'] for m in self.metrics) / len(self.metrics)
        
        return {
            'cpu_percent': avg_cpu,
            'memory_percent': avg_memory_percent,
            'memory_used_mb': avg_memory_mb
        }

# Global test configuration instance
TEST_CONFIG = TestConfig.from_env()

# Helper functions for test setup
def setup_integration_environment():
    """Setup integration test environment"""
    env_manager = TestEnvironmentManager(TEST_CONFIG)
    env_manager.setup()
    return env_manager

def create_test_client():
    """Create test client"""
    return APITestClient(TEST_CONFIG.api_base_url, TEST_CONFIG.api_timeout)

def generate_test_data():
    """Generate test data for integration tests"""
    return {
        'trends': TestDataGenerator.generate_sample_trends(15),
        'opportunities': TestDataGenerator.generate_sample_opportunities([], 8)
    }

def validate_test_response(response, expected_status: int = 200, required_fields: list = None):
    """Validate API response"""
    assert response.status_code == expected_status, f"Expected {expected_status}, got {response.status_code}"
    
    if required_fields and response.status_code == 200:
        try:
            data = response.json()
            for field in required_fields:
                assert field in data, f"Missing required field: {field}"
        except Exception as e:
            raise AssertionError(f"Response validation failed: {e}")

def measure_response_time(func):
    """Decorator to measure function response time"""
    import time
    from functools import wraps
    
    @wraps(func)
    def wrapper(*args, **kwargs):
        start_time = time.time()
        result = func(*args, **kwargs)
        end_time = time.time()
        
        response_time = end_time - start_time
        if hasattr(result, 'response_time'):
            result.response_time = response_time
        
        return result
    
    return wrapper

# Test configuration validation
def validate_test_config():
    """Validate test configuration"""
    issues = []
    
    # Check required directories exist or can be created
    try:
        Path(TEST_CONFIG.report_directory).mkdir(exist_ok=True)
    except Exception as e:
        issues.append(f"Cannot create report directory: {e}")
    
    # Check performance thresholds are reasonable
    if TEST_CONFIG.max_response_time <= 0:
        issues.append("max_response_time must be positive")
    
    if TEST_CONFIG.min_requests_per_second <= 0:
        issues.append("min_requests_per_second must be positive")
    
    # Check coverage percentage is valid
    if not (0 <= TEST_CONFIG.min_coverage_percentage <= 100):
        issues.append("min_coverage_percentage must be between 0 and 100")
    
    if issues:
        raise ValueError(f"Test configuration validation failed: {', '.join(issues)}")
    
    print("âœ… Test configuration validated")

# Initialize and validate configuration
if __name__ == '__main__':
    validate_test_config()
    print("ðŸ”§ Integration test configuration ready")
    print(f"   Database: {TEST_CONFIG.test_db_url}")
    print(f"   API URL: {TEST_CONFIG.api_base_url}")
    print(f"   Mock AI Services: {TEST_CONFIG.mock_ai_services}")
    print(f"   Mock External APIs: {TEST_CONFIG.mock_external_apis}")
    print(f"   Performance Thresholds: {TEST_CONFIG.max_response_time}s max, {TEST_CONFIG.min_requests_per_second} RPS min")