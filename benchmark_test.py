#!/usr/bin/env python3
"""
AI Content Factory - Performance Benchmark
à¸—à¸”à¸ªà¸­à¸šà¸›à¸£à¸°à¸ªà¸´à¸—à¸˜à¸´à¸ à¸²à¸à¸‚à¸­à¸‡à¸£à¸°à¸šà¸š
"""

import requests
import time
import threading
import statistics
import json

class PerformanceBenchmark:
    def __init__(self, base_url="http://localhost:5000"):
        self.base_url = base_url
        self.results = {}
    
    def benchmark_single_request(self, endpoint, method="GET", iterations=10):
        """à¸—à¸”à¸ªà¸­à¸šà¸›à¸£à¸°à¸ªà¸´à¸—à¸˜à¸´à¸ à¸²à¸à¸‚à¸­à¸‡ endpoint à¹€à¸”à¸µà¸¢à¸§"""
        print(f"ğŸ”„ Benchmarking {method} {endpoint}...")
        
        response_times = []
        success_count = 0
        
        for i in range(iterations):
            start_time = time.time()
            try:
                if method == "GET":
                    response = requests.get(f"{self.base_url}{endpoint}", timeout=10)
                elif method == "POST":
                    response = requests.post(f"{self.base_url}{endpoint}", timeout=10)
                
                end_time = time.time()
                response_time = (end_time - start_time) * 1000  # milliseconds
                
                if response.status_code == 200:
                    response_times.append(response_time)
                    success_count += 1
                    
            except Exception as e:
                print(f"   Request {i+1} failed: {e}")
        
        if response_times:
            avg_time = statistics.mean(response_times)
            min_time = min(response_times)
            max_time = max(response_times)
            median_time = statistics.median(response_times)
            
            result = {
                'endpoint': endpoint,
                'method': method,
                'iterations': iterations,
                'success_count': success_count,
                'success_rate': (success_count / iterations) * 100,
                'avg_response_time': avg_time,
                'min_response_time': min_time,
                'max_response_time': max_time,
                'median_response_time': median_time
            }
            
            self.results[f"{method} {endpoint}"] = result
            
            print(f"   âœ… Avg: {avg_time:.2f}ms | Min: {min_time:.2f}ms | Max: {max_time:.2f}ms")
            print(f"   ğŸ“Š Success Rate: {result['success_rate']:.1f}%")
            
        else:
            print(f"   âŒ All requests failed")
    
    def benchmark_concurrent_requests(self, endpoint, concurrent_users=5, requests_per_user=5):
        """à¸—à¸”à¸ªà¸­à¸š concurrent requests"""
        print(f"ğŸš€ Benchmarking concurrent access: {concurrent_users} users, {requests_per_user} requests each")
        
        all_response_times = []
        success_count = 0
        total_requests = concurrent_users * requests_per_user
        
        def user_simulation(user_id):
            user_times = []
            user_success = 0
            
            for req in range(requests_per_user):
                start_time = time.time()
                try:
                    response = requests.get(f"{self.base_url}{endpoint}", timeout=10)
                    end_time = time.time()
                    response_time = (end_time - start_time) * 1000
                    
                    if response.status_code == 200:
                        user_times.append(response_time)
                        user_success += 1
                        
                except Exception as e:
                    pass
            
            return user_times, user_success
        
        # à¸ªà¸£à¹‰à¸²à¸‡ threads
        threads = []
        results = [None] * concurrent_users
        
        start_time = time.time()
        
        for i in range(concurrent_users):
            thread = threading.Thread(
                target=lambda idx=i: results.__setitem__(idx, user_simulation(idx)),
                args=()
            )
            threads.append(thread)
            thread.start()
        
        # à¸£à¸­à¹ƒà¸«à¹‰ threads à¹€à¸ªà¸£à¹‡à¸ˆ
        for thread in threads:
            thread.join()
        
        total_time = time.time() - start_time
        
        # à¸£à¸§à¸¡à¸œà¸¥à¸¥à¸±à¸à¸˜à¹Œ
        for user_times, user_success in results:
            if user_times:
                all_response_times.extend(user_times)
                success_count += user_success
        
        if all_response_times:
            avg_time = statistics.mean(all_response_times)
            throughput = total_requests / total_time  # requests per second
            
            result = {
                'endpoint': endpoint,
                'concurrent_users': concurrent_users,
                'requests_per_user': requests_per_user,
                'total_requests': total_requests,
                'success_count': success_count,
                'success_rate': (success_count / total_requests) * 100,
                'total_time': total_time,
                'avg_response_time': avg_time,
                'throughput': throughput
            }
            
            self.results[f"Concurrent {endpoint}"] = result
            
            print(f"   âœ… Total Time: {total_time:.2f}s")
            print(f"   ğŸ“Š Throughput: {throughput:.2f} req/sec")
            print(f"   âš¡ Avg Response: {avg_time:.2f}ms")
            print(f"   ğŸ“ˆ Success Rate: {result['success_rate']:.1f}%")
        
    def benchmark_api_load(self):
        """à¸—à¸”à¸ªà¸­à¸š API load à¸•à¹ˆà¸²à¸‡à¹†"""
        print(f"âš¡ API Load Testing...")
        
        # à¸—à¸”à¸ªà¸­à¸šà¸‚à¸™à¸²à¸”à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸—à¸µà¹ˆà¸ªà¹ˆà¸‡à¸à¸¥à¸±à¸š
        endpoints_to_test = [
            ('/api/analytics', 'GET'),
            ('/api/trends', 'GET'),
            ('/api/opportunities', 'GET')
        ]
        
        for endpoint, method in endpoints_to_test:
            try:
                start_time = time.time()
                response = requests.get(f"{self.base_url}{endpoint}", timeout=10)
                response_time = (time.time() - start_time) * 1000
                
                if response.status_code == 200:
                    data_size = len(response.content)
                    
                    try:
                        json_data = response.json()
                        if endpoint == '/api/trends':
                            record_count = len(json_data.get('trends', []))
                        elif endpoint == '/api/opportunities':
                            record_count = len(json_data.get('opportunities', []))
                        else:
                            record_count = 1
                    except:
                        record_count = 0
                    
                    print(f"   ğŸ“„ {endpoint}: {data_size} bytes, {record_count} records, {response_time:.2f}ms")
                    
            except Exception as e:
                print(f"   âŒ {endpoint}: {e}")
    
    def run_full_benchmark(self):
        """à¸£à¸±à¸™à¸à¸²à¸£à¸—à¸”à¸ªà¸­à¸šà¸›à¸£à¸°à¸ªà¸´à¸—à¸˜à¸´à¸ à¸²à¸à¸—à¸±à¹‰à¸‡à¸«à¸¡à¸”"""
        print("âš¡ AI Content Factory - Performance Benchmark")
        print("=" * 60)
        
        # à¸—à¸”à¸ªà¸­à¸šà¸à¸²à¸£à¹€à¸Šà¸·à¹ˆà¸­à¸¡à¸•à¹ˆà¸­à¸à¸·à¹‰à¸™à¸à¸²à¸™
        try:
            response = requests.get(self.base_url, timeout=5)
            if response.status_code != 200:
                print("âŒ Server not responding properly")
                return False
        except:
            print("âŒ Cannot connect to server")
            return False
        
        print(f"ğŸ¯ Testing server: {self.base_url}")
        print("-" * 60)
        
        # 1. à¸—à¸”à¸ªà¸­à¸š individual endpoints
        print("\n1ï¸âƒ£ Individual Endpoint Performance:")
        endpoints = [
            ('/', 'GET'),
            ('/api/analytics', 'GET'),
            ('/api/trends', 'GET'),
            ('/api/opportunities', 'GET'),
            ('/api/collect-trends', 'POST'),
            ('/api/generate-opportunities', 'POST')
        ]
        
        for endpoint, method in endpoints:
            self.benchmark_single_request(endpoint, method, iterations=5)
            time.sleep(0.5)  # à¸«à¸™à¹ˆà¸§à¸‡à¹€à¸§à¸¥à¸²à¸£à¸°à¸«à¸§à¹ˆà¸²à¸‡à¸à¸²à¸£à¸—à¸”à¸ªà¸­à¸š
        
        # 2. à¸—à¸”à¸ªà¸­à¸š concurrent access
        print("\n2ï¸âƒ£ Concurrent Access Performance:")
        self.benchmark_concurrent_requests('/api/analytics', concurrent_users=3, requests_per_user=3)
        
        # 3. à¸—à¸”à¸ªà¸­à¸š API load
        print("\n3ï¸âƒ£ API Data Load Analysis:")
        self.benchmark_api_load()
        
        # 4. à¸—à¸”à¸ªà¸­à¸š stress test
        print("\n4ï¸âƒ£ Stress Testing:")
        self.stress_test()
        
        # à¹à¸ªà¸”à¸‡à¸ªà¸£à¸¸à¸›à¸œà¸¥
        self.print_summary()
        
        return True
    
    def stress_test(self):
        """à¸—à¸”à¸ªà¸­à¸šà¸„à¸§à¸²à¸¡à¸—à¸™à¸—à¸²à¸™à¸‚à¸­à¸‡à¸£à¸°à¸šà¸š"""
        print("ğŸ”¥ Running stress test (rapid requests)...")
        
        rapid_requests = 20
        success_count = 0
        start_time = time.time()
        
        for i in range(rapid_requests):
            try:
                response = requests.get(f"{self.base_url}/api/analytics", timeout=2)
                if response.status_code == 200:
                    success_count += 1
            except:
                pass
        
        total_time = time.time() - start_time
        throughput = rapid_requests / total_time
        success_rate = (success_count / rapid_requests) * 100
        
        print(f"   âš¡ {rapid_requests} rapid requests in {total_time:.2f}s")
        print(f"   ğŸ“Š Throughput: {throughput:.2f} req/sec")
        print(f"   âœ… Success Rate: {success_rate:.1f}%")
        
        if success_rate >= 90:
            print("   ğŸ‰ Excellent stress performance!")
        elif success_rate >= 70:
            print("   ğŸ˜Š Good stress performance")
        else:
            print("   âš ï¸ Poor stress performance - system may be overloaded")
    
    def print_summary(self):
        """à¹à¸ªà¸”à¸‡à¸ªà¸£à¸¸à¸›à¸œà¸¥à¸à¸²à¸£à¸—à¸”à¸ªà¸­à¸š"""
        print("\n" + "=" * 60)
        print("ğŸ“Š PERFORMANCE SUMMARY")
        print("=" * 60)
        
        if not self.results:
            print("âŒ No benchmark results available")
            return
        
        # à¸«à¸² endpoint à¸—à¸µà¹ˆà¹€à¸£à¹‡à¸§à¸—à¸µà¹ˆà¸ªà¸¸à¸”à¹à¸¥à¸°à¸Šà¹‰à¸²à¸—à¸µà¹ˆà¸ªà¸¸à¸”
        response_times = []
        for key, result in self.results.items():
            if 'avg_response_time' in result:
                response_times.append((key, result['avg_response_time']))
        
        if response_times:
            response_times.sort(key=lambda x: x[1])
            fastest = response_times[0]
            slowest = response_times[-1]
            
            print(f"âš¡ Fastest Endpoint: {fastest[0]} ({fastest[1]:.2f}ms)")
            print(f"ğŸŒ Slowest Endpoint: {slowest[0]} ({slowest[1]:.2f}ms)")
        
        # à¸„à¸³à¸™à¸§à¸“ overall performance score
        avg_response_times = [r['avg_response_time'] for r in self.results.values() if 'avg_response_time' in r]
        success_rates = [r['success_rate'] for r in self.results.values() if 'success_rate' in r]
        
        if avg_response_times and success_rates:
            avg_response = statistics.mean(avg_response_times)
            avg_success = statistics.mean(success_rates)
            
            # Performance score (lower response time = better, higher success rate = better)
            if avg_response < 100 and avg_success > 95:
                performance_grade = "A+ (Excellent)"
            elif avg_response < 200 and avg_success > 90:
                performance_grade = "A (Very Good)"
            elif avg_response < 500 and avg_success > 80:
                performance_grade = "B (Good)"
            elif avg_response < 1000 and avg_success > 70:
                performance_grade = "C (Average)"
            else:
                performance_grade = "D (Needs Improvement)"
            
            print(f"\nğŸ“ˆ Overall Performance Grade: {performance_grade}")
            print(f"â±ï¸ Average Response Time: {avg_response:.2f}ms")
            print(f"âœ… Average Success Rate: {avg_success:.1f}%")
        
        # à¹à¸™à¸°à¸™à¸³à¸à¸²à¸£à¸›à¸£à¸±à¸šà¸›à¸£à¸¸à¸‡
        print(f"\nğŸ’¡ Performance Recommendations:")
        
        slow_endpoints = [key for key, result in self.results.items() 
                         if 'avg_response_time' in result and result['avg_response_time'] > 500]
        
        if slow_endpoints:
            print(f"ğŸ”§ Optimize slow endpoints: {', '.join(slow_endpoints)}")
        
        low_success = [key for key, result in self.results.items() 
                      if 'success_rate' in result and result['success_rate'] < 90]
        
        if low_success:
            print(f"ğŸš¨ Fix reliability issues: {', '.join(low_success)}")
        
        if not slow_endpoints and not low_success:
            print("ğŸ‰ No performance issues detected!")
        
        # à¸£à¸²à¸¢à¸¥à¸°à¹€à¸­à¸µà¸¢à¸”à¹à¸•à¹ˆà¸¥à¸° endpoint
        print(f"\nğŸ“‹ Detailed Results:")
        print("-" * 40)
        for key, result in self.results.items():
            if 'avg_response_time' in result:
                print(f"{key}:")
                print(f"  â±ï¸ Response: {result['avg_response_time']:.2f}ms")
                print(f"  âœ… Success: {result['success_rate']:.1f}%")
                if 'throughput' in result:
                    print(f"  âš¡ Throughput: {result['throughput']:.2f} req/sec")


def main():
    """à¸Ÿà¸±à¸‡à¸à¹Œà¸Šà¸±à¸™à¸«à¸¥à¸±à¸"""
    import sys
    
    base_url = sys.argv[1] if len(sys.argv) > 1 else "http://localhost:5000"
    
    benchmark = PerformanceBenchmark(base_url)
    success = benchmark.run_full_benchmark()
    
    sys.exit(0 if success else 1)


if __name__ == "__main__":
    main()